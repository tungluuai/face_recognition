{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import libraries**"
      ],
      "metadata": {
        "id": "m1bZn_jIIjKJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCuBmdTySqXF",
        "outputId": "4891af9b-ecc6-4c4f-b5f6-fddd30157851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.225-py3-none-any.whl (660 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/660.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/660.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m655.4/660.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m660.1/660.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-facenet\n",
            "  Downloading keras-facenet-0.3.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Collecting mtcnn (from keras-facenet)\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.11.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn->keras-facenet) (2.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Building wheels for collected packages: keras-facenet\n",
            "  Building wheel for keras-facenet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-facenet: filename=keras_facenet-0.3.2-py3-none-any.whl size=10368 sha256=e7f32164186c6078a37e8ef68b0259b6e0c4cf4079ff261ffa2365e8d115272b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/d8/a9/85cf04ea29321d2afcb82c0caaafdca9195385f9d68cbc7185\n",
            "Successfully built keras-facenet\n",
            "Installing collected packages: mtcnn, thop, keras-facenet, ultralytics\n",
            "Successfully installed keras-facenet-0.3.2 mtcnn-0.1.1 thop-0.1.1.post2209072238 ultralytics-8.0.225\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics keras-facenet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "import random\n",
        "from keras_facenet import FaceNet\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.svm import SVC\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "pgTRjT3tS0SX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I27rHl2S1kp",
        "outputId": "eec7eba5-e02c-4397-d286-5ce3805aeb2d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load extra data**\n",
        "\n",
        "##├── extra_data\n",
        "###│   ├── < name_1 >\n",
        "####│   │   ├── images\n",
        "###│   ├── < name_2 >\n",
        "####│   │   ├── images\n",
        "###│   ├── < name_.. >\n",
        "####│   │   ├── images\n"
      ],
      "metadata": {
        "id": "r5wWOvC9IoKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load all file_paths of person to corresponding label\n",
        "def get_all_image_name_per_person(path):\n",
        "  all_name_files = os.listdir(path)\n",
        "  return all_name_files\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Biometrics/HW2/extra_data'\n",
        "all_folders = os.listdir(file_path)\n",
        "list_ = {}\n",
        "for folder in all_folders:\n",
        "  path_folder = os.path.join(file_path,folder)\n",
        "  all_files_per_person = get_all_image_name_per_person(path_folder)\n",
        "  list_[folder] = [os.path.join(path_folder, file) for file in all_files_per_person]"
      ],
      "metadata": {
        "id": "tAblD602T-_w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Detect faces and embed faces for extra data**"
      ],
      "metadata": {
        "id": "nAO_oQ99KOCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_image(original_image, coordinate):\n",
        "    cropped_image = original_image[int(coordinate[1]):int(coordinate[3]), int(coordinate[0]):int(coordinate[2])]\n",
        "    cropped_image = cv2.resize(cropped_image, (160,160))\n",
        "    return cropped_image"
      ],
      "metadata": {
        "id": "AqxgdDksXlt4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Load weights\n",
        "YOLO_model = YOLO('/content/drive/MyDrive/Biometrics/HW2/yolov8n-face.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk0RCxisX6VA",
        "outputId": "4b926f38-7794-400e-a72b-c0acb0d2be07"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ /content/drive/MyDrive/Biometrics/HW2/yolov8n-face.pt appears to require 'omegaconf', which is not in ultralytics requirements.\n",
            "AutoInstall will run now for 'omegaconf' but this feature will be removed in the future.\n",
            "Recommend fixes are to train a new model using the latest 'ultralytics' package or to run a command with an official YOLOv8 model, i.e. 'yolo predict model=yolov8n.pt'\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['omegaconf'] not found, attempting AutoUpdate...\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 2.6 MB/s eta 0:00:00\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.0/117.0 kB 8.6 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.1)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
            "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=b62d12d8c4c1c40d9a1fdecd7106d4c0c8a92172e5e1fe37fffa8c53d040a85c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_3e7dkr8/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 6.2s, installed 1 package: ['omegaconf']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_faces(data):\n",
        "  list_faces = list()\n",
        "  labels = list()\n",
        "  cnt = 0\n",
        "  for person in data:\n",
        "    for file in data[person]:\n",
        "      img = cv2.imread(file)\n",
        "      if img is None:\n",
        "        continue\n",
        "      face = YOLO_model(file, conf = 0.4)\n",
        "      coordinate_s = face[0].cpu().boxes.xyxy.tolist()\n",
        "      if len(coordinate_s) == 1:\n",
        "        [x1, y1, x2, y2] = coordinate_s[0]\n",
        "        cropped_image = crop_image(img, [x1, y1, x2, y2])\n",
        "        list_faces.append(cropped_image)\n",
        "        labels.append(person)\n",
        "  return np.asarray(list_faces), np.asarray(labels)"
      ],
      "metadata": {
        "id": "Ml6EqbXLX-9t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_images, extra_labels = detect_faces(list_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHhxK0_3YKmS",
        "outputId": "14efe837-7d3f-4beb-f337-0a39018f786f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/Biometrics/HW2/extra_data/Quy Tung/17a42fbf2120887ed131.jpg: 640x448 1 face, 51.0ms\n",
            "Speed: 12.7ms preprocess, 51.0ms inference, 48.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Biometrics/HW2/extra_data/Quy Tung/791fc85bc7c46e9a37d5.jpg: 640x544 1 face, 51.0ms\n",
            "Speed: 3.0ms preprocess, 51.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Biometrics/HW2/extra_data/Quy Tung/4180b0ceb851110f4840.jpg: 640x480 (no detections), 50.9ms\n",
            "Speed: 2.6ms preprocess, 50.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Biometrics/HW2/extra_data/Quy Tung/c95ab6b0be2f17714e3e.jpg: 640x480 1 face, 6.7ms\n",
            "Speed: 2.8ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Biometrics/HW2/extra_data/Quy Tung/287fd85b94c13d9f64d01.jpg: 640x480 1 face, 6.6ms\n",
            "Speed: 2.8ms preprocess, 6.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Biometrics/HW2/extra_data/Quy Tung/a8630f7443eeeab0b3ff3.jpg: 640x480 (no detections), 8.0ms\n",
            "Speed: 2.7ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Biometrics/HW2/extra_data/Quy Tung/517712765eecf7b2aefd4.jpg: 640x480 1 face, 6.4ms\n",
            "Speed: 2.6ms preprocess, 6.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Biometrics/HW2/extra_data/Quy Tung/c25a7791240b8d55d41a2.jpg: 640x480 1 face, 6.7ms\n",
            "Speed: 2.7ms preprocess, 6.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Biometrics/HW2/extra_data/Quy Tung/Screenshot 2023-12-09 123724.jpg: 640x544 1 face, 8.3ms\n",
            "Speed: 2.0ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 544)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = FaceNet()\n",
        "extra_embeddings = embedder.embeddings(extra_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZJjH1q6YsSm",
        "outputId": "e2a80896-fc9d-4082-b031-b799382fbc20"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_embedding = np.load('/content/drive/MyDrive/Biometrics/HW2/face_embedding/train_embeddings.npy')\n",
        "train_labels = np.load('/content/drive/MyDrive/Biometrics/HW2/data/train_labels.npy')"
      ],
      "metadata": {
        "id": "QBPs86s4Xln3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_embeddings = np.concatenate((train_embedding, extra_embeddings))\n",
        "train_labels = np.concatenate((train_labels, extra_labels))"
      ],
      "metadata": {
        "id": "C28pZZagY7Ns"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train**"
      ],
      "metadata": {
        "id": "x4pbQ_l_KaIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_encoder = Normalizer()\n",
        "emdTrainX_norm = in_encoder.transform(train_embedding)\n",
        "# label encode targets\n",
        "out_encoder = LabelEncoder()\n",
        "out_encoder.fit(train_labels)\n",
        "trainy_enc = out_encoder.transform(train_labels)\n",
        "# fit model\n",
        "model = SVC(kernel='linear', probability=True)\n",
        "model.fit(emdTrainX_norm, trainy_enc)"
      ],
      "metadata": {
        "id": "lYNhVoOkZy0v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "a17f855a-cbd4-4545-9401-8fab0b1faaa8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear', probability=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_names =  out_encoder.inverse_transform([i for i in range(0, len(set(train_labels)))])"
      ],
      "metadata": {
        "id": "NYh2Ew4sdpkh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###save for further use\n",
        "np.save('/content/drive/MyDrive/Biometrics/HW2/all_names.npy',all_names)\n",
        "support_vectors_file = '/content/drive/MyDrive/Biometrics/HW2/extra_SVM.pickle'\n",
        "pickle.dump(model, open(support_vectors_file, \"wb\"))"
      ],
      "metadata": {
        "id": "gZWzvYauZ9LY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Webcam**\n",
        "## *- Test on local machine*"
      ],
      "metadata": {
        "id": "vBVdj9uqacvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##load model\n",
        "YOLO_model = YOLO('yolov8n-face.pt')\n",
        "embedder = FaceNet()\n",
        "SVM_model = pickle.load(open('extra_SVM.pickle', 'rb'))"
      ],
      "metadata": {
        "id": "OXuHTy14ae8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(img, model):\n",
        "  samples = np.expand_dims(img, axis=0)\n",
        "  vector_embedding = embedder.embeddings(samples)\n",
        "  yhat_prob = model.predict_proba(vector_embedding)\n",
        "  return np.argmax(yhat_prob, axis=1)"
      ],
      "metadata": {
        "id": "sH6Un7lLNLHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_names = np.load('all_names.npy',allow_pickle=True)\n",
        "all_names = list(all_names.tolist())"
      ],
      "metadata": {
        "id": "_RLUsN8INMhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    # Đọc khung hình từ webcam\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    face = YOLO_model(frame, conf = 0.5)\n",
        "\n",
        "    coordinate = face[0].cpu().boxes.xyxy.tolist()\n",
        "    if len(coordinate) == 1:\n",
        "        [x1, y1, x2, y2] = coordinate[0]\n",
        "        cropped_image = crop_image(frame, [x1, y1, x2, y2])\n",
        "        result = predict(cropped_image, SVM_model)\n",
        "        name = all_names[result[0]]\n",
        "    # Hiển thị tên\n",
        "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, name, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    # Hiển thị video từ webcam\n",
        "    cv2.imshow('Face Recognition', frame)\n",
        "\n",
        "    # Thoát khỏi vòng lặp nếu người dùng nhấn phím 'q'\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Giải phóng webcam và đóng cửa sổ hiển thị\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "PqECHfVpNQX9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}